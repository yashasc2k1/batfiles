1) Switch on the VMware workstation.
2) Power on the Cloudera.
3) Switch on the Eclipse.
4) Goto File -> Switch workspace - >other.
5) Set the path to store the workspace.
6) Restore the workspace.
7) Goto File-> New-> Java Project.
8) Provide the title of the project, preferably as ‘WordCount’
9) Right click on the created project, New- >Class
10) Provide the name of class file, preferably as WC_Mapper
11) Create two more class files and provide the names as reducer and runner.
12) Write the program code.
13) Goto workspace created. Goto src folder.
14) Create ‘input.txt’ file to have the nay text data as an input to the program.
15) Right click on WordCount Project, goto properties.
16) Goto Java Build Path -> Libraries-> Add External JARs.
17)File System -> usr
18) Fetch the JAR files from 3 locations as
usr - > lib -> hadoop
usr - > lib -> hadoop -> lib
usr - > lib -> hadoop -> client0.20
19) Once the JAR files are attached, make sure all errors are rectified, and then RUN.
20) Executed successfully.
21) Goto workspace. Open the output folder. And a part file, you can see the final result.


*******************Program**********************

WC_Mapper.java:

import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter; public class WC_Mapper extends MapReduceBase implements Mapper<LongWritable,Text,Text,IntWritable>{
	private final static IntWritable one = new IntWritable(1);
	private Text word = new Text();
	public void map(LongWritable key, Text value,OutputCollector<Text,IntWritable> output,
		Reporter reporter) throws IOException{
		String line = value.toString();
		StringTokenizer tokenizer = new StringTokenizer(line);
	while (tokenizer.hasMoreTokens()){
		word.set(tokenizer.nextToken());
		output.collect(word, one);
		}
	}
}

WC_Reducer.java:

import java.io.IOException;
import java.util.Iterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text; import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;
	public class WC_Reducer extends MapReduceBase implements Reducer<Text,IntWritable,Text,IntWritable> {
	public void reduce(Text key, Iterator<IntWritable> values,OutputCollector<Text,IntWritable> output,
	Reporter reporter) throws IOException {
	int sum=0;
	while (values.hasNext()) {
		sum+=values.next().get();
	}
	output.collect(key,new IntWritable(sum));
	}
}

WC_Runner.java
import java.io.IOException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;
public class WC_Runner {
	public static void main(String[] args) throws IOException{
	JobConf conf = new JobConf(WC_Runner.class);
	conf.setJobName("WordCount");
	conf.setOutputKeyClass(Text.class);
	conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(WC_Mapper.class);
	conf.setCombinerClass(WC_Reducer.class);
	conf.setReducerClass(WC_Reducer.class);
	conf.setInputFormat(TextInputFormat.class);
	conf.setOutputFormat(TextOutputFormat.class);
	FileInputFormat.setInputPaths(conf,new Path("input.txt"));
	FileOutputFormat.setOutputPath(conf,new Path("output"));
	JobClient.runJob(conf);
	}
}

*****************END OF PROGRAM*********************